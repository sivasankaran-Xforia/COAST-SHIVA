#from typing import Dict, List
import os
import openai
import pandas as pd
#import torch
from transformers import AutoModelForTableQuestionAnswering, AutoTokenizer, pipeline
#import subprocess
#import json
from openai import OpenAI
#import ollama
from dotenv import load_dotenv 

load_dotenv()

# Get the API key from the environment
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise ValueError("OPENAI_API_KEY not found in environment variables.")

# Initialize the OpenAI client with the environment variable
client = OpenAI(api_key=OPENAI_API_KEY)



# Load model once
model_name = "google/tapas-large-finetuned-wtq"
#model_name = "google/tapas-base-finetuned-wtq"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForTableQuestionAnswering.from_pretrained(model_name)

tapas_qa_pipeline = pipeline(
    "table-question-answering",
    model=model,
    tokenizer=tokenizer,
    device=-1,  # Ensure it runs on CPU, as requested.
)

def process_chat_query(query: str, file_path: str, conversation_history: list[dict]=None) -> str:
    """
    Processes a user query using TAPAS for data extraction and a small LLM for conversational formatting,
    considering conversation history.

    Args:
        query (str): The current user's query.
        csv_path (str): Path to the CSV file.
        conversation_history (list[dict], optional): A list of dictionaries representing past turns.
                                                  Defaults to None (empty list).
    Returns:
        str: The conversational response generated by the hybrid system.
    """

    if conversation_history is None:
        conversation_history = [] 


    try:
        #df = pd.read_csv(csv_path)

        # IMPORTANT: Convert all DataFrame columns to string type for TAPAS compatibility.
        try:
            file_path_str= str(file_path)
            if file_path_str.endswith((".xlsx", ".xls")):
                df = pd.read_excel(file_path_str)
            else:
                df = pd.read_csv(file_path_str)
            print(df.head())
        except Exception as e:
            return f"Error reading file: {e}"
        
        df_str = df.astype(str)

        patient_names = df['Patient Name'].unique().tolist()
        query_lower = query.lower()

        matched_patients = [name for name in patient_names if name.lower() in query_lower]

        patient_query_attempted = False

        # Handle first-name-only queries
        if not matched_patients:
            first_names = [name.split()[0] for name in patient_names]
            for fn in first_names:
                if fn.lower() in query_lower:
                    patient_query_attempted = True # First name query attempted
                    fn_matches = df[df['Patient Name'].str.startswith(fn)]
                    if len(fn_matches) == 1:
                        matched_patients = [fn_matches.iloc[0]['Patient Name']]
                    elif len(fn_matches) > 1:
                        return "There are multiple patients with that first name. Please provide the full name."
    

        if matched_patients:
            # Patient-specific
            tapas_query = f"retrieve entire row for patient {matched_patients[0]}"
        elif patient_query_attempted:
            # Tried matching a first name but couldn't find a single match
            return "I could not find any matching patient in the data."
        else:
            # Table-level query (general stats)
            tapas_query = query

        contextualization_messages = [
            {
                "role": "system",
                "content": (
                    "You are a helpful assistant whose sole purpose is to rephrase a user's new query "
                    "into a complete, standalone question by incorporating information from the conversation history. "
                    "Do not add any new information. Just rephrase the query. " 
                    "Always include both first name and last name in the new query."
                    "Example: User says 'when did Mario Baker join?' You should output 'when did Mario Baker join'"
                    "Example: User says 'What about Michael?' after asking about ages. You should output 'What is Michael's age?'"
                    "Example: User says 'can you tell me more about Susan Farley' rephrase that as 'retrieve entire row for patient Susan Farley'"
                    "If the query is already complete, just return the original query. "
                    "Respond with ONLY the rephrased query. No conversational text."
                )
            }
        ]

        # Add past conversation turns
        contextualization_messages.extend(conversation_history)
        
        # Add the current query
        contextualization_messages.append({"role": "user", "content": tapas_query})

        try:
            contextualized_completion = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=contextualization_messages,
                temperature=0.0,
                max_tokens=200,
            )
            contextualized_query = contextualized_completion.choices[0].message.content.strip()
            print(f"Contextualized Query for TAPAS: {contextualized_query}") # For debugging
        except Exception as e:
            print(f"Error during query contextualization: {e}. Using original query.")
            contextualized_query = query



        # Use TAPAS to get a precise answer from the table
        tapas_result = tapas_qa_pipeline(table=df_str, query=contextualized_query)
        tapas_raw_answer = tapas_result['answer']
        
        # Prepare the TAPAS answer for the OpenAI LLM
        tapas_info_for_llm = ""
        if not tapas_raw_answer or tapas_raw_answer.strip().upper() == "NOT_FOUND" or tapas_raw_answer.strip() == "EMPTY":
            tapas_info_for_llm = "I couldn't find a direct answer in the table for your current question."
        else:
            tapas_info_for_llm = f"The precise information extracted from the patient data for the current query is: \"{tapas_raw_answer}\"."
        
        print(tapas_info_for_llm)

        # Construct Messages for OpenAI API
        messages = [
            {
                "role": "system",
                "content": (
                    "You are a concise medical assistant."
                    "If TAPAS has returned patient data, use it to answer the question."
                    "If no data is returned, say politely: 'I could not find any matching patient in the data.'"
                    "Do not fabricate any dates, names, or details. "
                    "Always keep responses crisp and factual."
                )
            }
        ]
        
        # Add past conversation turns to the messages
        messages.extend(conversation_history)

        # Add the TAPAS result as a user-like message, informing the LLM.
        messages.append({"role": "user", "content": f"[Precise Data Extractor Info]: {tapas_info_for_llm}"})
        messages.append({"role": "user", "content": f"Please generate a polite, concise, and conversational response based on all the above information."})


        try:
            # Make the API call to OpenAI
            chat_completion = client.chat.completions.create(
                model="gpt-4o-mini", 
                messages=messages,
                temperature=0.0, # Controls randomness: 0.0 (deterministic) to 1.0 (very creative)
                max_tokens=500, # Limit response length
            )
            response = chat_completion.choices[0].message.content.strip()
            return response

        except openai.APIConnectionError as e:
            return f"Error: Could not connect to OpenAI API. Please check your internet connection and API key. Details: {e}"
        except openai.RateLimitError as e:
            return f"Error: OpenAI API rate limit exceeded. Please wait a moment and try again. Details: {e}"
        except openai.APIStatusError as e:
            return f"Error from OpenAI API with status {e.status_code}: {e.response}. Check your API key or model availability."
        except Exception as e:
            return f"An unexpected error occurred while interacting with OpenAI API: {e}. Please try again."

    except FileNotFoundError:
        return f"Error: The CSV file was not found at the specified path: '{file_path_str}'. Please check the file path."
    except pd.errors.EmptyDataError:
        return f"Error: The CSV file at '{file_path_str}' is empty. Please ensure it contains data."
    except Exception as e:
        # Catch other potential errors during file reading or DataFrame conversion
        return f"An unexpected error occurred during data processing: {e}. Please review the CSV file format or the query."

